{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "bQh9CQX9xl-L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7cd83bac-f107-4fd2-fad7-cb28ef840437"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "initializing..\n",
            "loading text...\n",
            "tokenizing sentences...\n",
            "creating/loading vocab...\n",
            "creating dataset...\n",
            "initializing model...\n",
            "initializing optimizer and loss...\n",
            "training...\n",
            "it: 0  | loss 8.83  | Δw: 3.727\n",
            "it: 10  | loss 8.21  | Δw: 3.012\n",
            "it: 20  | loss 7.78  | Δw: 3.03\n",
            "it: 30  | loss 7.45  | Δw: 3.384\n",
            "it: 40  | loss 7.04  | Δw: 3.236\n",
            "it: 50  | loss 6.87  | Δw: 3.147\n",
            "it: 60  | loss 6.5  | Δw: 3.11\n",
            "it: 70  | loss 6.37  | Δw: 3.056\n",
            "it: 80  | loss 6.26  | Δw: 3.05\n",
            "it: 90  | loss 5.95  | Δw: 2.971\n",
            "it: 100  | loss 5.75  | Δw: 2.851\n",
            "it: 110  | loss 5.67  | Δw: 2.897\n",
            "it: 120  | loss 5.49  | Δw: 2.742\n",
            "it: 130  | loss 5.33  | Δw: 2.819\n",
            "it: 140  | loss 5.15  | Δw: 2.754\n",
            "it: 150  | loss 5.04  | Δw: 2.705\n",
            "it: 160  | loss 4.9  | Δw: 2.627\n",
            "it: 170  | loss 4.77  | Δw: 2.625\n",
            "it: 180  | loss 4.72  | Δw: 2.709\n",
            "it: 190  | loss 4.56  | Δw: 2.601\n",
            "it: 200  | loss 4.5  | Δw: 2.661\n",
            "it: 210  | loss 4.33  | Δw: 2.528\n",
            "it: 220  | loss 4.27  | Δw: 2.583\n",
            "it: 230  | loss 4.22  | Δw: 2.532\n",
            "it: 240  | loss 4.06  | Δw: 2.504\n",
            "it: 250  | loss 4.04  | Δw: 2.466\n",
            "it: 260  | loss 3.91  | Δw: 2.464\n",
            "it: 270  | loss 3.94  | Δw: 2.497\n",
            "it: 280  | loss 3.81  | Δw: 2.508\n",
            "it: 290  | loss 3.62  | Δw: 2.391\n",
            "it: 300  | loss 3.58  | Δw: 2.401\n",
            "it: 310  | loss 3.54  | Δw: 2.373\n",
            "it: 320  | loss 3.46  | Δw: 2.358\n",
            "it: 330  | loss 3.44  | Δw: 2.372\n",
            "it: 340  | loss 3.45  | Δw: 2.402\n",
            "it: 350  | loss 3.24  | Δw: 2.314\n",
            "it: 360  | loss 3.26  | Δw: 2.298\n",
            "it: 370  | loss 3.14  | Δw: 2.285\n",
            "it: 380  | loss 3.15  | Δw: 2.305\n",
            "it: 390  | loss 3.0  | Δw: 2.192\n",
            "it: 400  | loss 2.99  | Δw: 2.34\n",
            "it: 410  | loss 3.02  | Δw: 2.256\n",
            "it: 420  | loss 2.94  | Δw: 2.222\n",
            "it: 430  | loss 2.88  | Δw: 2.232\n",
            "it: 440  | loss 2.92  | Δw: 2.186\n",
            "it: 450  | loss 2.84  | Δw: 2.289\n",
            "it: 460  | loss 2.83  | Δw: 2.153\n",
            "it: 470  | loss 2.87  | Δw: 2.22\n",
            "it: 480  | loss 2.67  | Δw: 2.206\n",
            "it: 490  | loss 2.62  | Δw: 2.194\n",
            "it: 500  | loss 2.6  | Δw: 2.159\n",
            "it: 510  | loss 2.67  | Δw: 2.142\n",
            "it: 520  | loss 2.46  | Δw: 2.074\n",
            "it: 530  | loss 2.5  | Δw: 2.08\n",
            "it: 540  | loss 2.49  | Δw: 2.097\n",
            "it: 550  | loss 2.51  | Δw: 2.115\n",
            "it: 560  | loss 2.46  | Δw: 2.054\n",
            "it: 570  | loss 2.4  | Δw: 2.127\n",
            "it: 580  | loss 2.36  | Δw: 2.032\n",
            "it: 590  | loss 2.4  | Δw: 2.089\n",
            "it: 600  | loss 2.3  | Δw: 2.082\n",
            "it: 610  | loss 2.38  | Δw: 2.185\n",
            "it: 620  | loss 2.27  | Δw: 2.084\n",
            "it: 630  | loss 2.19  | Δw: 2.039\n",
            "it: 640  | loss 2.29  | Δw: 2.088\n",
            "it: 650  | loss 2.12  | Δw: 2.08\n",
            "it: 660  | loss 2.24  | Δw: 2.072\n",
            "it: 670  | loss 2.04  | Δw: 2.008\n",
            "it: 680  | loss 2.06  | Δw: 2.032\n",
            "it: 690  | loss 2.11  | Δw: 2.038\n",
            "it: 700  | loss 2.08  | Δw: 1.974\n",
            "it: 710  | loss 2.05  | Δw: 2.057\n",
            "it: 720  | loss 1.98  | Δw: 2.082\n",
            "it: 730  | loss 1.97  | Δw: 2.011\n",
            "it: 740  | loss 1.98  | Δw: 2.036\n",
            "it: 750  | loss 1.95  | Δw: 2.136\n",
            "it: 760  | loss 1.97  | Δw: 2.072\n",
            "it: 770  | loss 1.98  | Δw: 2.053\n",
            "it: 780  | loss 1.85  | Δw: 2.074\n",
            "it: 790  | loss 1.8  | Δw: 2.035\n",
            "it: 800  | loss 1.79  | Δw: 2.055\n",
            "it: 810  | loss 1.77  | Δw: 2.112\n",
            "it: 820  | loss 1.76  | Δw: 2.047\n",
            "it: 830  | loss 1.89  | Δw: 2.205\n",
            "it: 840  | loss 1.78  | Δw: 2.191\n",
            "it: 850  | loss 1.79  | Δw: 2.141\n",
            "it: 860  | loss 1.83  | Δw: 2.201\n",
            "it: 870  | loss 1.67  | Δw: 2.137\n",
            "it: 880  | loss 1.73  | Δw: 2.126\n",
            "it: 890  | loss 1.72  | Δw: 2.085\n",
            "it: 900  | loss 1.64  | Δw: 2.114\n",
            "it: 910  | loss 1.6  | Δw: 2.159\n",
            "it: 920  | loss 1.69  | Δw: 2.184\n",
            "it: 930  | loss 1.7  | Δw: 2.245\n",
            "it: 940  | loss 1.62  | Δw: 2.272\n",
            "it: 950  | loss 1.64  | Δw: 2.224\n",
            "it: 960  | loss 1.57  | Δw: 2.125\n",
            "it: 970  | loss 1.61  | Δw: 2.223\n",
            "it: 980  | loss 1.46  | Δw: 2.138\n",
            "it: 990  | loss 1.61  | Δw: 2.263\n",
            "saving embeddings...\n",
            "end\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# Libs\n",
        "# =============================================================================\n",
        "from torch.utils.data import Dataset\n",
        "import torch.nn.functional as F\n",
        "from collections import Counter\n",
        "from os.path import exists\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import math\n",
        "import re\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# Transformer\n",
        "# =============================================================================\n",
        "def attention(q, k, v, mask=None, dropout=None):\n",
        "    scores = q.matmul(k.transpose(-2, -1))\n",
        "    scores /= math.sqrt(q.shape[-1])\n",
        "\n",
        "    # mask\n",
        "    scores = scores if mask is None else scores.masked_fill(mask == 0, -1e3)\n",
        "\n",
        "    scores = F.softmax(scores, dim=-1)\n",
        "    scores = dropout(scores) if dropout is not None else scores\n",
        "    output = scores.matmul(v)\n",
        "    return output\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, n_heads, out_dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        #        self.q_linear = nn.Linear(out_dim, out_dim)\n",
        "        #        self.k_linear = nn.Linear(out_dim, out_dim)\n",
        "        #        self.v_linear = nn.Linear(out_dim, out_dim)\n",
        "        self.linear = nn.Linear(out_dim, out_dim * 3)\n",
        "\n",
        "        self.n_heads = n_heads\n",
        "        self.out_dim = out_dim\n",
        "        self.out_dim_per_head = out_dim // n_heads\n",
        "        self.out = nn.Linear(out_dim, out_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def split_heads(self, t):\n",
        "        return t.reshape(t.shape[0], -1, self.n_heads, self.out_dim_per_head)\n",
        "\n",
        "    def forward(self, x, y=None, mask=None):\n",
        "        # in decoder, y comes from encoder. In encoder, y=x\n",
        "        y = x if y is None else y\n",
        "\n",
        "        qkv = self.linear(x)  # BS * SEQ_LEN * (3*EMBED_SIZE_L)\n",
        "        q = qkv[:, :, :self.out_dim]  # BS * SEQ_LEN * EMBED_SIZE_L\n",
        "        k = qkv[:, :, self.out_dim:self.out_dim * 2]  # BS * SEQ_LEN * EMBED_SIZE_L\n",
        "        v = qkv[:, :, self.out_dim * 2:]  # BS * SEQ_LEN * EMBED_SIZE_L\n",
        "\n",
        "        # break into n_heads\n",
        "        q, k, v = [self.split_heads(t) for t in (q, k, v)]  # BS * SEQ_LEN * HEAD * EMBED_SIZE_P_HEAD\n",
        "        q, k, v = [t.transpose(1, 2) for t in (q, k, v)]  # BS * HEAD * SEQ_LEN * EMBED_SIZE_P_HEAD\n",
        "\n",
        "        # n_heads => attention => merge the heads => mix information\n",
        "        scores = attention(q, k, v, mask, self.dropout)  # BS * HEAD * SEQ_LEN * EMBED_SIZE_P_HEAD\n",
        "        scores = scores.transpose(1, 2).contiguous().view(scores.shape[0], -1,\n",
        "                                                          self.out_dim)  # BS * SEQ_LEN * EMBED_SIZE_L\n",
        "        out = self.out(scores)  # BS * SEQ_LEN * EMBED_SIZE\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, inp_dim, inner_dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(inp_dim, inner_dim)\n",
        "        self.linear2 = nn.Linear(inner_dim, inp_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # inp => inner => relu => dropout => inner => inp\n",
        "        return self.linear2(self.dropout(F.relu(self.linear1(x))))\n",
        "\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, n_heads, inner_transformer_size, inner_ff_size, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.mha = MultiHeadAttention(n_heads, inner_transformer_size, dropout)\n",
        "        self.ff = FeedForward(inner_transformer_size, inner_ff_size, dropout)\n",
        "        self.norm1 = nn.LayerNorm(inner_transformer_size)\n",
        "        self.norm2 = nn.LayerNorm(inner_transformer_size)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        x2 = self.norm1(x)\n",
        "        x = x + self.dropout1(self.mha(x2, mask=mask))\n",
        "        x2 = self.norm2(x)\n",
        "        x = x + self.dropout2(self.ff(x2))\n",
        "        return x\n",
        "\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, n_code, n_heads, embed_size, inner_ff_size, n_embeddings, seq_len, dropout=.1):\n",
        "        super().__init__()\n",
        "\n",
        "        # model input\n",
        "        self.embeddings = nn.Embedding(n_embeddings, embed_size)\n",
        "        self.pe = PositionalEmbedding(embed_size, seq_len)\n",
        "\n",
        "        # backbone\n",
        "        encoders = []\n",
        "        for i in range(n_code):\n",
        "            encoders += [EncoderLayer(n_heads, embed_size, inner_ff_size, dropout)]\n",
        "        self.encoders = nn.ModuleList(encoders)\n",
        "\n",
        "        # language model\n",
        "        self.norm = nn.LayerNorm(embed_size)\n",
        "        self.linear = nn.Linear(embed_size, n_embeddings, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embeddings(x)\n",
        "        x = x + self.pe(x)\n",
        "        for encoder in self.encoders:\n",
        "            x = encoder(x)\n",
        "        x = self.norm(x)\n",
        "        x = self.linear(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Positional Embedding\n",
        "class PositionalEmbedding(nn.Module):\n",
        "    def __init__(self, d_model, max_seq_len=80):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        pe = torch.zeros(max_seq_len, d_model)\n",
        "        pe.requires_grad = False\n",
        "        for pos in range(max_seq_len):\n",
        "            for i in range(0, d_model, 2):\n",
        "                pe[pos, i] = math.sin(pos / (10000 ** ((2 * i) / d_model)))\n",
        "                pe[pos, i + 1] = math.cos(pos / (10000 ** ((2 * (i + 1)) / d_model)))\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.pe[:, :x.size(1)]  # x.size(1) = seq_len\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# Dataset\n",
        "# =============================================================================\n",
        "class SentencesDataset(Dataset):\n",
        "    # Init dataset\n",
        "    def __init__(self, sentences, vocab, seq_len):\n",
        "        dataset = self\n",
        "\n",
        "        dataset.sentences = sentences\n",
        "        dataset.vocab = vocab + ['<ignore>', '<oov>', '<noise>']\n",
        "        dataset.vocab = {e: i for i, e in enumerate(dataset.vocab)}\n",
        "        dataset.rvocab = {v: k for k, v in dataset.vocab.items()}\n",
        "        dataset.seq_len = seq_len\n",
        "\n",
        "        # special tags\n",
        "        dataset.IGNORE_IDX = dataset.vocab['<ignore>']  # replacement tag for tokens to ignore\n",
        "        dataset.OUT_OF_VOCAB_IDX = dataset.vocab['<oov>']  # replacement tag for unknown words\n",
        "        dataset.NOISE_IDX = dataset.vocab['<noise>']  # replacement tag for the masked word prediction task\n",
        "\n",
        "    # fetch data\n",
        "    def __getitem__(self, index, p_random_mask=0.15):\n",
        "        dataset = self\n",
        "\n",
        "        # while we don't have enough word to fill the sentence for a batch\n",
        "        s = []\n",
        "        while len(s) < dataset.seq_len:\n",
        "            s.extend(dataset.get_sentence_idx(index % len(dataset)))\n",
        "            index += 1\n",
        "\n",
        "        # ensure that the sequence is of length seq_len\n",
        "        s = s[:dataset.seq_len]\n",
        "        [s.append(dataset.IGNORE_IDX) for i in range(dataset.seq_len - len(s))]  # PAD ok\n",
        "        # apply random mask\n",
        "        s = [(dataset.NOISE_IDX, w) if random.random() < p_random_mask else (w, w) for w in s]\n",
        "\n",
        "        return {'input': torch.Tensor([w[0] for w in s]).long(),\n",
        "                'target': torch.Tensor([w[1] for w in s]).long()}\n",
        "\n",
        "    # return length\n",
        "    def __len__(self):\n",
        "        return len(self.sentences)\n",
        "\n",
        "    # get words id\n",
        "    def get_sentence_idx(self, index):\n",
        "        dataset = self\n",
        "        s = dataset.sentences[index]\n",
        "        s = [dataset.vocab[w] if w in dataset.vocab else dataset.OUT_OF_VOCAB_IDX for w in s]\n",
        "        return s\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# Methods / Class\n",
        "# =============================================================================\n",
        "def get_batch(loader, loader_iter):\n",
        "    try:\n",
        "        batch = next(loader_iter)\n",
        "    except StopIteration:\n",
        "        loader_iter = iter(loader)\n",
        "        batch = next(loader_iter)\n",
        "    return batch, loader_iter\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# #Init\n",
        "# =============================================================================\n",
        "print('initializing..')\n",
        "batch_size = 128\n",
        "seq_len = 20\n",
        "embed_size = 128\n",
        "inner_ff_size = embed_size * 4\n",
        "n_heads = 8\n",
        "n_code = 8\n",
        "n_vocab = 40000\n",
        "dropout = 0.1\n",
        "# n_workers = 12\n",
        "# optimizer\n",
        "optim_kwargs = {'lr': 1e-4, 'weight_decay': 1e-4, 'betas': (.9, .999)}\n",
        "\n",
        "# =============================================================================\n",
        "# Input\n",
        "# =============================================================================\n",
        "# 1) load text\n",
        "print('loading text...')\n",
        "pth = 'train_noi.txt'\n",
        "sentences = open(pth,errors=\"ignore\").read().lower().split('\\n')\n",
        "# 2) tokenize sentences (can be done during training, you can also use spacy udpipe)\n",
        "print('tokenizing sentences...')\n",
        "special_chars = ',,.\"?;.:/*!+-()[]{}\"\\'&'\n",
        "sentences = [re.sub(r'[^\\w\\s]', '',s).split(' ') for s in sentences]\n",
        "sentences = [[w for w in s if len(w)] for s in sentences]\n",
        "\n",
        "# 3) create vocab if not already created\n",
        "print('creating/loading vocab...')\n",
        "pth = 'vocab.txt'\n",
        "if not exists(pth):\n",
        "    words = [w for s in sentences for w in s]\n",
        "    vocab = Counter(words).most_common(n_vocab)  # keep the N most frequent words\n",
        "    vocab = [w[0] for w in vocab]\n",
        "    open(pth, 'w+').write('\\n'.join(vocab))\n",
        "else:\n",
        "    vocab = open(pth).read().split('\\n')\n",
        "\n",
        "# 4) create dataset\n",
        "print('creating dataset...')\n",
        "dataset = SentencesDataset(sentences, vocab, seq_len)\n",
        "# kwargs = {'num_workers':n_workers, 'shuffle':True,  'drop_last':True, 'pin_memory':True, 'batch_size':batch_size}\n",
        "kwargs = {'shuffle': True, 'drop_last': True, 'pin_memory': True, 'batch_size': batch_size}\n",
        "data_loader = torch.utils.data.DataLoader(dataset, **kwargs)\n",
        "\n",
        "# =============================================================================\n",
        "# Model\n",
        "# =============================================================================\n",
        "# init model\n",
        "print('initializing model...')\n",
        "model = Transformer(n_code, n_heads, embed_size, inner_ff_size, len(dataset.vocab), seq_len, dropout)\n",
        "#model = model.cuda()\n",
        "\n",
        "# =============================================================================\n",
        "# Optimizer\n",
        "# =============================================================================\n",
        "print('initializing optimizer and loss...')\n",
        "optimizer = optim.Adam(model.parameters(), **optim_kwargs)\n",
        "loss_model = nn.CrossEntropyLoss(ignore_index=dataset.IGNORE_IDX)\n",
        "\n",
        "# =============================================================================\n",
        "# Train\n",
        "# =============================================================================\n",
        "print('training...')\n",
        "print_each = 10\n",
        "model.train()\n",
        "batch_iter = iter(data_loader)\n",
        "n_iteration = 1000\n",
        "for it in range(n_iteration):\n",
        "\n",
        "    # get batch\n",
        "    batch, batch_iter = get_batch(data_loader, batch_iter)\n",
        "\n",
        "    # infer\n",
        "    masked_input = batch['input']\n",
        "    masked_target = batch['target']\n",
        "\n",
        "    #masked_input = masked_input.cuda(non_blocking=True)\n",
        "    #masked_target = masked_target.cuda(non_blocking=True)\n",
        "    output = model(masked_input)\n",
        "\n",
        "    # compute the cross entropy loss\n",
        "    output_v = output.view(-1, output.shape[-1])\n",
        "    target_v = masked_target.view(-1, 1).squeeze()\n",
        "    loss = loss_model(output_v, target_v)\n",
        "\n",
        "    # compute gradients\n",
        "    loss.backward()\n",
        "\n",
        "    # apply gradients\n",
        "    optimizer.step()\n",
        "\n",
        "    # print step\n",
        "    if it % print_each == 0:\n",
        "        print('it:', it,\n",
        "              ' | loss', np.round(loss.item(), 2),\n",
        "              ' | Δw:', round(model.embeddings.weight.grad.abs().sum().item(), 3))\n",
        "\n",
        "    # reset gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "# =============================================================================\n",
        "# Results analysis\n",
        "# =============================================================================\n",
        "print('saving embeddings...')\n",
        "N = 5743\n",
        "np.savetxt('values.tsv', np.round(model.embeddings.weight.detach().cpu().numpy()[0:N], 2), delimiter='\\t', fmt='%1.2f')\n",
        "s = [dataset.rvocab[i] for i in range(N)]\n",
        "open('names.tsv', 'w+').write('\\n'.join(s))\n",
        "\n",
        "print('end')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#============================================================================\n",
        "# Input\n",
        "# =============================================================================\n",
        "# 1) load text\n",
        "batch_size=1\n",
        "def predict(pth):\n",
        "  sentences =pth.lower().split('\\n')\n",
        "  # 2) tokenize sentences (can be done during training, you can also use spacy udpipe)\n",
        "  #print('tokenizing sentences...')\n",
        "  special_chars = ',?;.:/*!+-()[]{}\"\\'&'\n",
        "  sentences = [re.sub(r'[^\\w\\s]', '',s).split(' ') for s in sentences]\n",
        "  sentences = [[w for w in s if len(w)] for s in sentences]\n",
        "\n",
        "  # 3) create vocab if not already created\n",
        "  #print('creating/loading vocab...')\n",
        "  pth = 'vocab.txt'\n",
        "  if not exists(pth):\n",
        "      words = [w for s in sentences for w in s]\n",
        "      vocab = Counter(words).most_common(n_vocab)  # keep the N most frequent words\n",
        "      vocab = [w[0] for w in vocab]\n",
        "      open(pth, 'w+').write('\\n'.join(vocab))\n",
        "  else:\n",
        "      vocab = open(pth).read().split('\\n')\n",
        "\n",
        "  # 4) create dataset\n",
        "  #print('creating dataset...')\n",
        "  test_dataset = SentencesDataset(sentences, vocab, seq_len)\n",
        "  # kwargs = {'num_workers':n_workers, 'shuffle':True,  'drop_last':True, 'pin_memory':True, 'batch_size':batch_size}\n",
        "  kwargs = {'shuffle': True, 'drop_last': True, 'pin_memory': True, 'batch_size': batch_size}\n",
        "  test_data_loader = torch.utils.data.DataLoader(test_dataset, **kwargs)\n",
        "\n",
        "  #print('testing')\n",
        "  print_each = 10\n",
        "\n",
        "  batch_iter = iter(test_data_loader)\n",
        "\n",
        "  # get batch\n",
        "  batch, batch_iter = get_batch(test_data_loader, batch_iter)\n",
        "\n",
        "  # infer\n",
        "  masked_input = batch['input']\n",
        "  masked_target = batch['target']\n",
        "\n",
        "  #masked_input = masked_input.cuda(non_blocking=True)\n",
        "  #masked_target = masked_target.cuda(non_blocking=True)\n",
        "  output = model(masked_input)\n",
        "\n",
        "  # compute the cross entropy loss\n",
        "  output_v = output.view(-1, output.shape[-1])\n",
        "  target_v = masked_target.view(-1, 1).squeeze()\n",
        "  output = F.log_softmax(output_v, dim=-1)\n",
        "  max=output.argmax(1)\n",
        "  print(\"output:\")\n",
        "  print(' '.join([vocab[i] for i in max]))\n"
      ],
      "metadata": {
        "id": "AWTMmgkPP6z0"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print('loading testing text...')\n",
        "pth = ['The model used an uncertainty metric to determine whether patient data were too different from what it had been trained on for it to be able to make a successful prediction',\n",
        "       'Here are some critical AI terms that you need to know [we will be updating this glossary on a regular basis, so we recommend checking in on it routinely',\n",
        "       'A set of instructions or rules used - often by a computer - to solve a set of problems, execute calculations or process data',\n",
        "       'the process of using data analysis to deduce properties of an underlying probability distribution. Inferential statistical analysis infers properties of a population, for example by testing hypotheses and deriving estimates.  It is assumed that the observed data set is sampled from a larger population. Inferential statistics can be contrasted with descriptive statistics. Descriptive statistics is solely concerned with properties of the observed data',\n",
        "       'it does not rest on the assumption that the data come from a larger population',\n",
        "       'Promoter analysis involves the identification and study of sequence motifs in the DNA surrounding the coding region of a gene. These motifs influence the extent to which that region is transcribed into mRNA. Enhancer elements far away from the promoter can also regulate gene expression',\n",
        "        'In between them are zero or more hidden layers. Single layer and unlayered networks are also used'\n",
        "       ]\n",
        "\n",
        "for p in pth:\n",
        "  print(\"Input:\")\n",
        "  print(p)\n",
        "  predict(p)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9tSMJmmCoRv",
        "outputId": "5bcabd43-2a7b-4344-9d79-7c033a6404c7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:\n",
            "The model used an uncertainty metric to determine whether patient data were too different from what it had been trained on for it to be able to make a successful prediction\n",
            "output:\n",
            "the model used an uncertainty used to determine whether is data and too different from what it had been trained\n",
            "Input:\n",
            "Here are some critical AI terms that you need to know [we will be updating this glossary on a regular basis, so we recommend checking in on it routinely\n",
            "output:\n",
            "here and some critical ai to that you need that know to of be updating this glossary on a regular\n",
            "Input:\n",
            "A set of instructions or rules used - often by a computer - to solve a set of problems, execute calculations or process data\n",
            "output:\n",
            "a set of instructions or a used often by a a to solve a a of a a calculations or\n",
            "Input:\n",
            "the process of using data analysis to deduce properties of an underlying probability distribution. Inferential statistical analysis infers properties of a population, for example by testing hypotheses and deriving estimates.  It is assumed that the observed data set is sampled from a larger population. Inferential statistics can be contrasted with descriptive statistics. Descriptive statistics is solely concerned with properties of the observed data\n",
            "output:\n",
            "the process of using data analysis to is properties of an underlying the distribution inferential statistical analysis the properties of\n",
            "Input:\n",
            "it does not rest on the assumption that the data come from a larger population\n",
            "output:\n",
            "it does not vision on the assumption that the data come from a users population of does and vision on\n",
            "Input:\n",
            "Promoter analysis involves the identification and study of sequence motifs in the DNA surrounding the coding region of a gene. These motifs influence the extent to which that region is transcribed into mRNA. Enhancer elements far away from the promoter can also regulate gene expression\n",
            "output:\n",
            "promoter analysis involves the identification and study of sequence artificial in the dna variables the when statistics of the gene\n",
            "Input:\n",
            "In between them are zero or more hidden layers. Single layer and unlayered networks are also used\n",
            "output:\n",
            "in between them are between or more hidden layers single layer and lot networks are also used in between them\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WG_jOm-_X4sD"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iitADUhnevim"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
